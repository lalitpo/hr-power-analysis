{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve all the relevant data from the table athletic-data in the database.\n",
    "# Pre-requisite :\n",
    "  1. Activity has to be of more than 2hrs.\n",
    "  2. Watts and HeartRate column should not be null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T17:31:38.680443Z",
     "start_time": "2024-01-14T17:31:29.753874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-01-14 18:31:38 Strava properties loaded.\n",
      "\u001B[32m2024-01-14 18:31:38 Database connection properties loaded.\n",
      "\u001B[32m2024-01-14 18:31:38 Application properties loaded.\n",
      "\u001B[32m2024-01-14 18:31:38 You are successfully connected to hr-power-db database!\n",
      "\u001B[32m2024-01-14 18:31:38 DB Schema created successfully\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import pandas as pd\n",
    "\n",
    "from src.config.LoadProperties import retrieve_data_sql\n",
    "from src.repositories.PowerAndHRRepository import configs, sql_engine\n",
    "year = 2021\n",
    "athlete_id = 863203\n",
    "athlete_name=\"Connor_new\"\n",
    "\n",
    "\"\"\"\n",
    "Read and return athletic data from a SQL database.\n",
    "\n",
    ":return: A pandas DataFrame containing the athletic data.\n",
    "\"\"\"\n",
    "\n",
    "def get_athletic_data(id, activity_year):\n",
    "    return pd.read_sql(retrieve_data_sql.replace(\"(athlete_id)\",str(id)+\")\").replace(\"(year)\",str(activity_year)), sql_engine)\n",
    "\n",
    "athletic_data_db = get_athletic_data(athlete_id, year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DATA PROFILING : Removing irrelevant columns not required for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T17:31:56.655483Z",
     "start_time": "2024-01-14T17:31:56.637407Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.config.LoadProperties import remove_columns\n",
    "\n",
    "athletic_data = athletic_data_db.drop(remove_columns.split(\",\"), axis=1).rename(columns={'watts' : 'power'})\n",
    "athletic_data['distance'] = athletic_data['distance'].apply(lambda x: [float(value) for value in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check if the lengths of lists in specified columns are the same for each row\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of activity IDs with different lengths: 0\n"
     ]
    }
   ],
   "source": [
    "params = ['heartrate','power', 'cadence', 'distance', 'time','velocity_smooth']\n",
    "\n",
    "def check_lengths(row):\n",
    "    lengths = [len(row[col]) for col in params]\n",
    "    return all(length == lengths[0] for length in lengths)\n",
    "\n",
    "result = athletic_data[athletic_data.apply(check_lengths, axis=1)]\n",
    "count_different_lengths = len(athletic_data) - len(result)\n",
    "\n",
    "print(f\"Count of activity IDs with different lengths: {count_different_lengths}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T17:32:17.609655Z",
     "start_time": "2024-01-14T17:32:17.593785Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NA/NONE CHECK : Checking if power, distance, time and heartrate column contains None/NaN values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1 entries contains None in heartrate\n",
      "0/1 entries contains NaN in heartrate\n",
      "1/1 entries contains None in power\n",
      "1/1 entries contains NaN in power\n",
      "0/1 entries contains None in cadence\n",
      "0/1 entries contains NaN in cadence\n",
      "0/1 entries contains None in distance\n",
      "0/1 entries contains NaN in distance\n",
      "0/1 entries contains None in time\n",
      "0/1 entries contains NaN in time\n",
      "0/1 entries contains None in velocity_smooth\n",
      "0/1 entries contains NaN in velocity_smooth\n"
     ]
    }
   ],
   "source": [
    "def check_nan_entries(data, param):\n",
    "    none_count = data.apply(lambda x: any(item is None for item in x) if x is not None else False).sum()\n",
    "    print(f\"{none_count}/{len(data)} entries contains None in \"  + param)\n",
    "    nan_count = data.apply(lambda x: any(pd.isna(item) for item in x) if x is not None else False).sum()\n",
    "    print(f\"{nan_count}/{len(data)} entries contains NaN in \"  + param)\n",
    "\n",
    "for i, val in enumerate(params):\n",
    "    check_nan_entries(athletic_data[val], val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T17:32:21.314494Z",
     "start_time": "2024-01-14T17:32:21.300024Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Continuity Check : Checking if data in time column in evenly spaced/incremented by only 1.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def check_increment(row):\n",
    "#     if not all(b == a + 1 for a, b in zip(row['time'][:-1], row['time'][1:])):\n",
    "#         return row['activity_id']\n",
    "#     return None\n",
    "# \n",
    "# abc['result'] = athletic_data.apply(check_increment, axis=1)\n",
    "# \n",
    "# # The 'result' column will contain the 'activity_id' where the values are not incremented by 1\n",
    "# filtered_result = athletic_data['result'].dropna().astype(int)\n",
    "# \n",
    "# print(filtered_result)\n",
    "# \n",
    "# result_dataframe = athletic_data[athletic_data['activity_id'] == 6399104761][['time']].explode('time', ignore_index=True)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cluster view of how long the NA/None/Null entries exists."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "athletic_data_with_na = pd.DataFrame(index=athletic_data['activity_id'], columns=range(len(max(athletic_data['power'], key=len))))\n",
    " \n",
    "for idx, powers in zip(athletic_data['activity_id'], athletic_data['power']):\n",
    "    athletic_data_with_na.loc[idx, :len(powers) - 1] = pd.Series(powers).isna().values\n",
    "\n",
    "athletic_data_with_na = athletic_data_with_na.astype(float)\n",
    "\n",
    "cmap = sns.color_palette(['#00FF00', '#FF0000'])\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(data=athletic_data_with_na, cmap=cmap, cbar=False, xticklabels=False) \n",
    "plt.title('Visualization of NA/None/Null Values in Power Column')\n",
    "plt.xlabel('Activity Length(in seconds)')\n",
    "plt.ylabel('Activity ID')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualization of the length of NA/None/Null Values in Power Column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "\n",
    "cons_na_pow_list = athletic_data['power'].apply(lambda x: [0] + [len(list(g)) for k, g in groupby(x) if pd.isna(k)]).tolist()\n",
    "\n",
    "def count_occurrences(na_list):\n",
    "    flat_list = [min(num, 60) for sublist in na_list for num in sublist if num != 0]\n",
    "    occurrences = Counter(flat_list)\n",
    "    return dict(occurrences)\n",
    "\n",
    "seq_na_occ_count = count_occurrences(cons_na_pow_list)\n",
    "\n",
    "plt.figure(figsize=(30, 12))\n",
    "\n",
    "plt.bar(seq_na_occ_count.keys(), seq_na_occ_count.values(), color='blue', edgecolor='black')\n",
    "\n",
    "for key, value in seq_na_occ_count.items():\n",
    "    plt.text(key, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "\n",
    "plt.title('Occurrence Count Histogram')\n",
    "plt.xlabel('Consecutive Time(in seconds)')\n",
    "plt.ylabel('Sequential NA Occurrence Count') \n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Interpolation : Applying Linear Interpolation only for NA/None occurring for maximum 5 seconds in the power data recorded."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "na_limit = 5\n",
    "\n",
    "def interpolate_nan(sublist):\n",
    "\n",
    "    inter_list = pd.Series(sublist).interpolate(limit_direction='both').tolist()[1:] \n",
    "    rd_inter_list = [round(i_val) for i_val in inter_list]\n",
    "    \n",
    "    return rd_inter_list\n",
    "\n",
    "\n",
    "def interpolate_none_values(orig_list, na_limit):\n",
    "    interpolated_list = []\n",
    "    count = 0\n",
    "    for i, val in enumerate(orig_list):\n",
    "        if val is None:\n",
    "            if len(orig_list)-1 != i:                                                  # if na_occurrence not at the end of the list\n",
    "                count += 1\n",
    "            else:                                                                     # if na_occurrence is at the end of the list\n",
    "                interpolated_list.extend(orig_list[-count:])\n",
    "                interpolated_list.append(val) \n",
    "        else:\n",
    "            if na_limit >= count > 0 != i-count:                                        # when found non-NA before and after NA AND na_occurrence <= na_limit\n",
    "                sublist = orig_list[i - count - 1 : i + 1 ]\n",
    "                interpolated_list.extend(interpolate_nan(sublist)) \n",
    "                count = 0\n",
    "            elif (na_limit >= count > 0 == i-count) or count > na_limit:              # when series start with NA and non-NA found after NA AND na_occurrence <= na_limit\n",
    "                interpolated_list.extend([None] * count)\n",
    "                interpolated_list.append(val)\n",
    "                count = 0\n",
    "            else:                                                                     # when another non-NA found after non-NA\n",
    "                interpolated_list.append(val)\n",
    "            \n",
    "    return interpolated_list\n",
    "\n",
    "interpolated_power = [interpolate_none_values(seq, na_limit) for seq in athletic_data['power']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T17:32:38.002895Z",
     "start_time": "2024-01-14T17:32:37.963229Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adding Index column of segment starting index\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "athletic_data['power_ip'] = interpolated_power\n",
    "\n",
    "#athletic_data['power_ip_na_adjusted'] = athletic_data['power_ip'].apply(lambda x: [x[i] for i in range(len(x)) if pd.notna(x[i])])\n",
    "\n",
    "\n",
    "#list_a = [1, 4, 5, None, None, None, 4, 5, 9, None, 4, 5, None, None, 3, 8, 10, None]\n",
    "#[1, 4, 5, 4, 5, 9, 4, 5, 3, 8, 10]\n",
    "#[0, 1, 2, 6, 7, 8, 10, 11, 14, 15, 16]\n",
    "#[0, 3, 6, 8]\n",
    "#list_ax = [None, 1, 4, 5, None, None, None, 4, 5, None, 2, 3, 9, None, None, 3, 8, 10, None]\n",
    "#[1, 4, 5, 4, 5, 2, 3, 9, 3, 8, 10]\n",
    "#[1, 2, 3, 7, 8, 10, 11, 12, 15, 16, 17]\n",
    "#[0, 3, 5, 8]\n",
    "def find_segment_start_indices(original_list):\n",
    "    non_none_indices = [i for i, val in enumerate(original_list) if pd.notna(val)]\n",
    "\n",
    "    segment_index_list = []\n",
    "    sub_index = []\n",
    "    for i in range(0, len(non_none_indices)):\n",
    "        if i == 0  and non_none_indices[i] >= 0:\n",
    "            sub_index.append(non_none_indices[i]+1)\n",
    "        if non_none_indices[i] > non_none_indices[i-1] + 1:\n",
    "            sub_index.append(non_none_indices[i-1]+1)\n",
    "            segment_index_list.append(sub_index)\n",
    "            sub_index = [non_none_indices[i]+1]\n",
    "        if i == len(non_none_indices)-1:\n",
    "            sub_index.append(non_none_indices[i]+1)\n",
    "            segment_index_list.append(sub_index)\n",
    "    return segment_index_list\n",
    "\n",
    "\n",
    "athletic_data['segments'] = athletic_data.apply(lambda row: find_segment_start_indices(row['power_ip']), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T17:32:40.920905Z",
     "start_time": "2024-01-14T17:32:40.908901Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NA Replacement Check"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from itertools import groupby\n",
    "# \n",
    "# consec_na_len = 7\n",
    "# athletic_data['consec_na_power'] = athletic_data['power'].apply(lambda x: [0] + [len(list(g)) for k, g in groupby(x) if pd.isna(k)])\n",
    "# athletic_data['consec_na_power_ip'] = athletic_data['power_ip'].apply(lambda x: [0] + [len(list(g)) for k, g in groupby(x) if pd.isna(k)])\n",
    "# \n",
    "# rows_with_cons_na = athletic_data[athletic_data['consec_na_power'].apply(lambda x: consec_na_len in x)]\n",
    "# rows_with_cons_na_ip = athletic_data[athletic_data['consec_na_power_ip'].apply(lambda x: consec_na_len in x)]\n",
    "# \n",
    "# if not rows_with_cons_na.empty:\n",
    "#     athletic_ids_with_na = rows_with_cons_na['activity_id'].tolist()\n",
    "#     print(f\"Activity IDs with {consec_na_len} consecutive NAs: {athletic_ids_with_na}\")\n",
    "# if not rows_with_cons_na_ip.empty:\n",
    "#      athletic_ids_with_na_ip = rows_with_cons_na_ip['activity_id'].tolist()\n",
    "#      print(f\"Activity IDs with {consec_na_len} consecutive NAs: {athletic_ids_with_na_ip}\")\n",
    "# else:\n",
    "#     print(\"No rows with \"+ consec_na_len + \"consecutive NAs found.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Position of the NA in the power list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# activity_id_to_extract = 6399104761\n",
    "# \n",
    "# selected_activity = athletic_data[athletic_data['activity_id'] == activity_id_to_extract]\n",
    "# \n",
    "# orig_power_df = pd.DataFrame({'power': selected_activity['power'].iloc[0]})\n",
    "# power_df_ip = pd.DataFrame({'power_ip': selected_activity['power_ip'].iloc[0]})\n",
    "# segment_index = pd.DataFrame({'segment_start_index': selected_activity['segment_start_index'].iloc[0]})\n",
    "# NA_comparison_df =  pd.concat([orig_power_df, power_df_ip, segment_index], axis=1)\n",
    "# \n",
    "# \n",
    "# activity_id_to_extract = 6399104761\n",
    "# \n",
    "# selected_activity_2 = athletic_data[athletic_data['activity_id'] == activity_id_to_extract]\n",
    "# \n",
    "# orig_power_df_2 = pd.DataFrame({'power': selected_activity_2['power'].iloc[0]})\n",
    "# orig_hr_df_2 = pd.DataFrame({'hr': selected_activity_2['heartrate'].iloc[0]})\n",
    "# orig_time_df_2 = pd.DataFrame({'time': selected_activity_2['time'].iloc[0]})\n",
    "# pow_ip_df = pd.DataFrame({'power_ip': selected_activity_2['power_ip'].iloc[0]})\n",
    "# \n",
    "# NA_comparison_df_2 =  pd.concat([orig_power_df_2, orig_hr_df_2, orig_time_df_2, pow_ip_df], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Choosing the longest continuous segments of each of the rides"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def find_longest_segment_indices(lst):\n",
    "    if not lst:\n",
    "        return [None, None]\n",
    "\n",
    "    start_index, end_index = 0, 0\n",
    "    current_start, current_end = 0, 0\n",
    "    max_length = 1\n",
    "\n",
    "    for i in range(1, len(lst)):\n",
    "        if lst[i] == lst[i - 1] + 1:\n",
    "            current_end = i\n",
    "        else:\n",
    "            current_start = i\n",
    "            current_end = i\n",
    "\n",
    "        current_length = current_end - current_start + 1\n",
    "\n",
    "        if current_length > max_length:\n",
    "            start_index, end_index = current_start, current_end\n",
    "            max_length = current_length\n",
    "\n",
    "    return [start_index, end_index]\n",
    "\n",
    "#athletic_data['longest_time_segment'] = athletic_data['time'].apply(find_longest_segment_indices)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculating datapoints of all the activities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "get_list_length = lambda x: len(x) if isinstance(x, list) else 0\n",
    "\n",
    "athletic_data['datapoints'] = athletic_data['power_ip'].apply(get_list_length)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of True and False: moving\n",
      "True     10691\n",
      "False       46\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "inspected_activity_id = 6406834713\n",
    "\n",
    "inspected_activity = athletic_data[athletic_data['activity_id'] == inspected_activity_id]\n",
    "\n",
    "orig_hr_df = pd.DataFrame({'hr': inspected_activity['heartrate'].iloc[0]})\n",
    "orig_time_df = pd.DataFrame({'time': inspected_activity['time'].iloc[0]})\n",
    "orig_distance_df = pd.DataFrame({'distance': inspected_activity['distance'].iloc[0]})\n",
    "orig_power_df = pd.DataFrame({'orig_power': inspected_activity['power'].iloc[0]})\n",
    "power_ip_df = pd.DataFrame({'power_ip': inspected_activity['power_ip'].iloc[0]})\n",
    "cadence_df = pd.DataFrame({'cadence': inspected_activity['cadence'].iloc[0]})\n",
    "moving_df = pd.DataFrame({'moving': inspected_activity['moving'].iloc[0]})\n",
    "counts = moving_df['moving'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(\"Counts of True and False:\", counts)\n",
    "velocity_smooth_df = pd.DataFrame({'velocity_smooth': inspected_activity['velocity_smooth'].iloc[0]})\n",
    "\n",
    "inspected_activity_df =  pd.concat([orig_time_df, orig_distance_df, orig_hr_df, moving_df, cadence_df, orig_power_df, power_ip_df, velocity_smooth_df], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T17:55:21.806247Z",
     "start_time": "2024-01-14T17:55:21.724417Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Converting Dataframe into CSV for further analysis in MATLAB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(athletic_data)\n",
    "\n",
    "\n",
    "csv_directory = f'Strava Data'\n",
    "csv_path = f'{csv_directory}/{athlete_name}_{athlete_id}_{year}.csv'\n",
    "\n",
    "if not os.path.exists(csv_directory):\n",
    "    os.makedirs(csv_directory)\n",
    "\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "print(f\"DataFrame has been successfully converted to CSV and saved at: {csv_path}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
